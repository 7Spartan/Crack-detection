{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wW3rPmhsyeSm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhLTWj-StjaP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import os\n",
    "import glob\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2U8Si_z6tjaY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1M2NFzmFtjaa"
   },
   "outputs": [],
   "source": [
    "def segment_img(img,stride,npw,nph):\n",
    "    img = cv2.imread(img)\n",
    "    height,width, color = img.shape\n",
    "    test = 0\n",
    "    # print(height,width,color)\n",
    "    orig = img\n",
    "    num_strides_height = height/stride\n",
    "    num_strides_width = width/stride\n",
    "    rows = int(num_strides_height)\n",
    "    columns = int(num_strides_width)\n",
    "    # print(rows,columns)\n",
    "    for r in range(rows):\n",
    "        for c in range(columns):\n",
    "            x1 = (c)*stride\n",
    "            y1 = (r)*stride\n",
    "            x2 = x1+npw\n",
    "            y2 = y1+nph\n",
    "            if test == 0:\n",
    "                in_img = img[y1:y2,x1:x2]\n",
    "                if(len(in_img.shape)<3):\n",
    "                    in_img = in_img.reshape(1,224,224,1)\n",
    "                    in_img = in_img/255\n",
    "                else:\n",
    "                    in_img = in_img.astype('uint8')\n",
    "                    in_img = cv2.cvtColor(in_img,cv2.COLOR_RGB2GRAY)\n",
    "                    in_img = in_img.reshape(1,224,224,1)\n",
    "                    in_img = in_img/255\n",
    "#The below line is used to predict\n",
    "                result = model.predict(in_img, batch_size=None, verbose=0, steps=None)\n",
    "                re = result[0]\n",
    "                if re[0]>0.8:\n",
    "                    print('cracked')\n",
    "                    print('confidence: ',re[0]*100,'%')\n",
    "                    orig = cv2.rectangle(orig,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "                else:\n",
    "                    jkl=0\n",
    "                    # print('no crack')\n",
    "            if ((width-x2 >= npw) or (width==npw)):\n",
    "                test = 0\n",
    "            else:\n",
    "                break\n",
    "        if ((height-y2) >= nph or (height==nph)):\n",
    "            test=0\n",
    "        else:\n",
    "            break\n",
    "    return(orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 483,
     "status": "error",
     "timestamp": 1558217855207,
     "user": {
      "displayName": "rajat paliwal",
      "photoUrl": "",
      "userId": "15052888110978584031"
     },
     "user_tz": 420
    },
    "id": "eumwHpCltjad",
    "outputId": "df696616-1204-45c1-9d04-d6541add2e69",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad argument type for built-in operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dbab86de5054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Our operations on the frame come here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnpw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Display the resulting frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2f678fd34350>\u001b[0m in \u001b[0;36msegment_img\u001b[0;34m(img, stride, npw, nph)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msegment_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnpw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(height,width,color)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad argument type for built-in operation"
     ]
    }
   ],
   "source": [
    "import torch._utils\n",
    "try:\n",
    "    torch._utils._rebuild_tensor_v2\n",
    "except AttributeError:\n",
    "    def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n",
    "        tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)\n",
    "        tensor.requires_grad = requires_grad\n",
    "        tensor._backward_hooks = backward_hooks\n",
    "        return tensor\n",
    "    torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2\n",
    "path = '/home/jerin/fastai/files/34stage_2.pth'\n",
    "model = torch.load(path,map_location='cuda:0')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "stride = 50 #pixels to stride frwd or down\n",
    "#Shape of neural net dectectable image width x height\n",
    "neural_pixels_width = 224\n",
    "neural_pixels_height = 224\n",
    "npw = neural_pixels_width\n",
    "nph = neural_pixels_height\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    results = segment_img(frame,stride,npw,nph)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',results)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kv0kvxjytjai"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RP_Prototype_V3.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/7Spartan/Real-time-crack-deteection/blob/master/Prototype_V3.ipynb",
     "timestamp": 1558217389836
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
